{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comic Genre Classifier by Zach & Brooke"
      ],
      "metadata": {
        "id": "g2fnibSa-aSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Webtoons for URLs of Comic Episodes"
      ],
      "metadata": {
        "id": "pSPw9G_O-7ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import httpx\n",
        "import asyncio\n",
        "import aiofiles\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import nest_asyncio\n",
        "import time\n",
        "import random\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "semaphore = asyncio.Semaphore(100)\n",
        "\n",
        "pd.set_option('display.width', 100)"
      ],
      "metadata": {
        "id": "6yVg3bcZ-53h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_page_url_with_genres_dict():\n",
        "    webtoons_comic_page_urls = []\n",
        "    webtoons_comic_genre = []\n",
        "\n",
        "    originals_url = \"https://www.webtoons.com/en/originals\"\n",
        "    r = requests.get(originals_url)\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "    completed_comics = soup.find('div', class_='daily_section on').find_all('a', href=True)\n",
        "\n",
        "    webtoons_comic_page_urls += [a['href'] for a in completed_comics]\n",
        "    webtoons_comic_genre += [comic.find('p').text for comic in completed_comics]\n",
        "\n",
        "    ongoing_comics_days = soup.find_all('div', class_=re.compile('daily_section\\s+_list_\\w*'))\n",
        "    for ongoing_comic_day in ongoing_comics_days:\n",
        "        daily_ongoing_comics = ongoing_comic_day.find_all('li')\n",
        "\n",
        "        webtoons_comic_page_urls += [li.find('a', href=True)['href'] for li in daily_ongoing_comics]\n",
        "        webtoons_comic_genre += [comic.find('p').text for comic in daily_ongoing_comics]\n",
        "\n",
        "    return dict(zip(webtoons_comic_page_urls, webtoons_comic_genre))\n",
        "\n",
        "def create_episode_url_with_genres_dict(page_url_genre_dict):\n",
        "    episode_url_genre_dict = {}\n",
        "    # Scrape episode URLs for each comic page URL\n",
        "    for comic_page_url, genre in page_url_genre_dict.items():\n",
        "        r = requests.get(comic_page_url)\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        for li in soup.find_all('li', class_='_episodeItem'):\n",
        "            episode_url = li.find('a', href=True)['href']\n",
        "            episode_url_genre_dict[episode_url] = genre\n",
        "    return episode_url_genre_dict\n",
        "\n",
        "page_url_genre_dict = create_page_url_with_genres_dict()\n",
        "episode_url_genre_dict = create_episode_url_with_genres_dict(page_url_genre_dict)"
      ],
      "metadata": {
        "id": "Wsg7_Hd3_LDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading JPG files of comic page images to disk sorted into directories based on genre\n",
        "\n",
        "import random\n",
        "async def download_image(url, directory, filename, semaphore):\n",
        "    querystring = {\"type\": \"q90\"}\n",
        "    headers = {\"referer\": \"https://www.webtoons.com/\"}\n",
        "\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        async with semaphore:\n",
        "            response = await client.get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                async with aiofiles.open(os.path.join(directory, filename), \"wb\") as f:\n",
        "                    await f.write(response.content)\n",
        "\n",
        "async def main():\n",
        "    start_time = time.time()\n",
        "    image_directory = 'images'\n",
        "    if not os.path.exists(image_directory):\n",
        "        os.makedirs(image_directory)\n",
        "    # Download images for each episode URL asynchronously\n",
        "    tasks = []\n",
        "    counter = 0\n",
        "    for episode_url, genre in list(episode_url_genre_dict.items())[0:len(episode_url_genre_dict):2]:\n",
        "        genre_directory = f\"{image_directory}/{genre}\"\n",
        "        if not os.path.exists(genre_directory):\n",
        "            os.makedirs(genre_directory)\n",
        "        async with httpx.AsyncClient() as client:\n",
        "            r = await client.get(episode_url)\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        for img in list(soup.find_all('img', class_='_images'))[0:100:8]:\n",
        "            img_url = re.sub(\"\\?type=\\w*\", \"\", img['data-url'])\n",
        "            filename = img_url.split('/')[-1]\n",
        "            tasks.append(download_image(img_url, genre_directory, filename, semaphore))  # Pass semaphore here\n",
        "        counter += 1\n",
        "        print(f\"finished {episode_url} {counter} {time.time() - start_time}\")\n",
        "    print(time.time() - start_time)\n",
        "    await asyncio.gather(*tasks)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "na3GKt6__WdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Images From the Images Directory For OCR & Writing Features and Labels to Disk After Formatting"
      ],
      "metadata": {
        "id": "KxKWSH7z-h3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/opt/local/bin/tesseract'\n",
        "\n",
        "img_data = []\n",
        "text_data = []\n",
        "genre_data = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "Path = 'images'\n",
        "for genre in os.listdir(Path):\n",
        "    genre_start_time = time.time()\n",
        "    if genre == '.DS_Store':\n",
        "        continue\n",
        "    path = os.path.join(Path, genre)\n",
        "    for images in os.listdir(path):\n",
        "        if images == '.DS_Store':\n",
        "            continue\n",
        "        path1 = os.path.join(path,images)\n",
        "        image = Image.open(path1)\n",
        "        text = pytesseract.image_to_string(image).strip()\n",
        "        if text:\n",
        "            text_data.append(text)\n",
        "            genre_data.append(genre)\n",
        "            image = image.resize((100,125))\n",
        "            image = image.convert('RGB')\n",
        "            img_data.append(np.array(image))\n",
        "    print(f\"finished {genre} in {time.time()-genre_start_time}\")\n",
        "\n",
        "import pickle\n",
        "\n",
        "# write list to binary file\n",
        "def write_list(filename, input_list):\n",
        "    # store list in binary file so 'wb' mode\n",
        "    with open(filename, 'wb') as fp:\n",
        "        pickle.dump(input_list, fp)\n",
        "\n",
        "np.save('img_data_new', np.array(img_data).astype('int8'))\n",
        "write_list('text_data_new.bin', text_data)\n",
        "write_list('genre_data_new.bin', genre_data)\n",
        "\n",
        "print(f\"finished everything in {time.time()-start_time}\")"
      ],
      "metadata": {
        "id": "pfFH4FE8-ZlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: Uploaded files written to disk above into Google Drive so that we could work with them on Google Colab"
      ],
      "metadata": {
        "id": "n8gBfKs6Artp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing & Training Neural Network"
      ],
      "metadata": {
        "id": "gWR5HBU1-WKw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHDGcI9b-Kr4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "userdata.get('HuggingFace')\n",
        "\n",
        "def create_data_folder(user): # Different path to folder depending on user\n",
        "    if user.lower() == 'brooke':\n",
        "        return '/content/drive/MyDrive/Final Project'\n",
        "    if user.lower() == 'zach':\n",
        "        return '/content/drive/MyDrive/Year 2/Spring 2024/DS340/FinalProject'\n",
        "    return '/content/drive/MyDrive/Final Project'\n",
        "\n",
        "# Define the path to the folder containing the data\n",
        "drive.mount('/content/drive')\n",
        "data_folder = create_data_folder('zach') #change to 'brooke' when you use it\n",
        "\n",
        "# neural network classifier\n",
        "class GenreClassifier(nn.Module):\n",
        "    def __init__(self, text_input_size, image_input_size, hidden_size, num_classes):\n",
        "        super(GenreClassifier, self).__init__()\n",
        "        self.text_fc1 = nn.Linear(text_input_size, hidden_size)\n",
        "        self.image_fc1 = nn.Linear(image_input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, text_x, image_x):\n",
        "        text_out = self.relu(self.text_fc1(text_x))\n",
        "        image_out = self.relu(self.image_fc1(image_x))\n",
        "        combined = torch.cat((text_out, image_out), dim=1)\n",
        "        out = self.fc2(combined)\n",
        "        return out\n",
        "\n",
        "# DistilBERT model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# NN layer that does nothing (used to replace output layer of ResNet50)\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "# Load ResNet50 model\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet.eval()\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "resnet.fc = Identity()\n",
        "\n",
        "# preprocessing for text\n",
        "def get_tokens(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# preprocessing for images\n",
        "preprocess_image = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# extract image features using ResNet50\n",
        "def extract_image_features_resnet(img_arr):\n",
        "    image = Image.fromarray(np.uint8(img_arr))\n",
        "    image = preprocess_image(image).unsqueeze(0) #shape(1,3,224,224)\n",
        "    with torch.no_grad():\n",
        "        features = resnet(image) # shape (1,2048)\n",
        "    features = torch.nn.functional.relu(features)  # ReLU activation ; shape (1,2048)\n",
        "    #features = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))  # Global average pooling\n",
        "    features = features.squeeze() # shape (2048)\n",
        "    return features\n",
        "\n",
        "# helper function for load_data() for reading binary files\n",
        "def read_bin(filename):\n",
        "    with open(filename, 'rb') as fp:\n",
        "        n_list = pickle.load(fp)\n",
        "        return n_list\n",
        "\n",
        "# load data\n",
        "def load_data():\n",
        "    img_data = np.load('/content/drive/MyDrive/Year 2/Spring 2024/DS340/FinalProject/img_data2.npy', allow_pickle=True)\n",
        "    text_data = read_bin('/content/drive/MyDrive/Year 2/Spring 2024/DS340/FinalProject/text_data.bin')\n",
        "    genre_data = read_bin('/content/drive/MyDrive/Year 2/Spring 2024/DS340/FinalProject/genre_data.bin')\n",
        "    return img_data, text_data, genre_data\n",
        "\n",
        "# combine features and labels\n",
        "# def combine_features_and_labels(img_data, text_data):\n",
        "#     since = time.time()\n",
        "#     text_features_list = []\n",
        "#     image_features_list = []\n",
        "#     counter = 0\n",
        "#     for img in img_data:\n",
        "#         image_features = extract_image_features_resnet(img).detach().numpy()\n",
        "#         image_features_list.append(image_features)\n",
        "#         if counter%100 == 0:\n",
        "#             print(f\"{counter/len(img_data)*100}% done with images\")\n",
        "#         counter += 1\n",
        "#     print(f\"images finished: {time.time()-since}\")\n",
        "#     since = time.time()\n",
        "#     counter = 0\n",
        "#     for text in text_data:\n",
        "#         tokens = get_tokens(text)\n",
        "#         outputs = model(**tokens)\n",
        "#         text_features = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
        "#         text_features_list.append(text_features)\n",
        "#         if counter%100 == 0:\n",
        "#             print(f\"{counter/len(text_data)*100}% done with text\")\n",
        "#         counter += 1\n",
        "#     print(f\"text finished: {time.time()-since}\")\n",
        "\n",
        "#     text_features_array = np.array(text_features_list)\n",
        "#     image_features_array = np.array(image_features_list)\n",
        "\n",
        "#     print(f\"{text_features_array.nbytes} bytes in text_array\")\n",
        "#     print(f\"{image_features_array.nbytes} bytes in image_array\")\n",
        "\n",
        "#     # flatten\n",
        "#     text_features_flat = text_features_array.reshape(-1)\n",
        "#     image_features_flat = image_features_array.reshape(-1)\n",
        "\n",
        "#     # concatenate\n",
        "#     combined_features = np.concatenate((text_features_flat, image_features_flat), axis=0)\n",
        "\n",
        "#     return combined_features\n",
        "\n",
        "def combine_features_and_labels(img_data, text_data, batch_size):\n",
        "    since = time.time()\n",
        "    combined_features_list = []\n",
        "    for i in range(0, len(img_data), batch_size):\n",
        "        img_batch = img_data[i:i+batch_size]\n",
        "        text_batch = text_data[i:i+batch_size]\n",
        "\n",
        "        image_features_batch = []\n",
        "        text_features_batch = []\n",
        "        for img in img_batch:\n",
        "            image_features = extract_image_features_resnet(img).detach().numpy()\n",
        "            image_features_batch.append(image_features)\n",
        "\n",
        "        for text in text_batch:\n",
        "            tokens = get_tokens(text)\n",
        "            outputs = model(**tokens)\n",
        "            text_features = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
        "            text_features_batch.append(text_features)\n",
        "\n",
        "        mean_image_features = np.mean(image_features_batch, axis=0)\n",
        "        mean_text_features = np.mean(text_features_batch, axis=0)\n",
        "\n",
        "        combined_features = np.concatenate((mean_text_features, mean_image_features))\n",
        "        combined_features_list.append(combined_features)\n",
        "\n",
        "        print(f\"{i / len(img_data) * 100}% done with images and text\")\n",
        "\n",
        "    combined_features_array = np.array(combined_features_list)\n",
        "\n",
        "    print(f\"{combined_features_array.nbytes} bytes in combined features array\")\n",
        "\n",
        "    return combined_features_array\n",
        "\n",
        "# Load and combine data\n",
        "batch_size = 4\n",
        "img_data, text_data, genre_data = load_data()\n",
        "img_data = img_data\n",
        "text_data = text_data\n",
        "labels = genre_data[::batch_size]\n",
        "label_encodings = dict(zip(np.unique(labels), np.arange(len(np.unique(labels)))))\n",
        "labels = [*map(label_encodings.get, labels)]\n",
        "features = combine_features_and_labels(img_data, text_data, batch_size)\n",
        "\n",
        "# Split data into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train)\n",
        "print(X_train.shape)\n",
        "print(y_train)\n",
        "\n",
        "# Convert data to tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# params\n",
        "input_size_text = X_train.shape[1] // 2  # Assuming text and image features have the same size\n",
        "input_size_image = X_train.shape[1] // 2\n",
        "hidden_size = 128\n",
        "num_classes = len(np.unique(labels))\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "classifier = GenreClassifier(input_size_text, input_size_image, hidden_size, num_classes)\n",
        "\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "# training\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        text_inputs = inputs[:, :input_size_text]  # Extract text features\n",
        "        image_inputs = inputs[:, input_size_text:]  # Extract image features\n",
        "        outputs = classifier(text_inputs, image_inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(f\"Training finished in {time.time() - start_time} seconds.\")\n",
        "\n",
        "# evaluation\n",
        "# evaluation\n",
        "with torch.no_grad():\n",
        "    outputs = classifier(X_test_tensor[:, :input_size_text], X_test_tensor[:, input_size_text:])\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    test_accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    }
  ]
}